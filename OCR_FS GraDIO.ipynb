{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UwcIFwjg4IN-",
    "outputId": "38b2035b-40ad-46b6-a809-e04096efc6b0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\h.decure\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import fnmatch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.activations import relu, sigmoid, softmax\n",
    "import keras.backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from PIL import Image,ImageFont,ImageDraw\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import gradio as gr\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "from SegmentPage import segment_into_lines\n",
    "from SegmentLine import segment_into_words\n",
    "from RecognizeWord import recognize_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples = 150000, word_type = 'all', progress=gr.Progress()):\n",
    "    global gray_back\n",
    "    kernel=np.ones((2,2),np.uint8)\n",
    "    kernel2=np.ones((1,1),np.uint8)\n",
    "    punclist=',.?!:;'\n",
    "\n",
    "    #Character sets to choose from.\n",
    "    smallletters=string.ascii_lowercase\n",
    "    capitalletters=string.ascii_uppercase\n",
    "    digits=string.digits\n",
    "    all_char = smallletters+capitalletters+digits+punclist\n",
    "    \n",
    "    #Base backgound.\n",
    "    backfilelist=os.listdir('./background/')\n",
    "    backgroud_list=[]\n",
    "\n",
    "    for bn in backfilelist:\n",
    "        fileloc='./background/'+bn\n",
    "        backgroud_list.append(Image.fromarray(cv2.imread(fileloc,0)))\n",
    "\n",
    "\n",
    "    #Different fonts to be used.\n",
    "    fonts_list=os.listdir('./fonts/')\n",
    "    fonts_list=['./fonts/'+f for f in fonts_list]\n",
    "\n",
    "    #Lengths of the words.\n",
    "    word_lengths=[]\n",
    "    for l in range(1,21):\n",
    "        word_lengths.append(l)\n",
    "\n",
    "    #Font size.\n",
    "    font_size=[]\n",
    "    for l in range(10,30):\n",
    "        font_size.append(l)\n",
    "\n",
    "\n",
    "    file_counter=0\n",
    "\n",
    "\n",
    "    def random_brightness(img):\n",
    "        img=np.array(img)\n",
    "        brightness=iaa.Multiply((0.2,1.2))\n",
    "        img=brightness.augment_image(img)\n",
    "        return img\n",
    "\n",
    "    def dilation(img):\n",
    "        img=np.array(img)\n",
    "        img=cv2.dilate(img,kernel2,iterations=1)\n",
    "        return img\n",
    "\n",
    "    def erosion(img):\n",
    "        img=np.array(img)\n",
    "        img=cv2.erode(img,kernel,iterations=1)\n",
    "        return img\n",
    "\n",
    "    def blur(img):\n",
    "        img=np.array(img)\n",
    "        img=cv2.blur(img,ksize=(3,3))\n",
    "\n",
    "\n",
    "\n",
    "    def fuse_gray(img):\n",
    "        img=np.array(img)\n",
    "        ht,wt=img.shape[0],img.shape[1]\n",
    "        gray_back=cv2.imread('gray_back.jpg',0)\n",
    "        gray_back=cv2.resize(gray_back,(wt,ht))\n",
    "\n",
    "        blended=cv2.addWeighted(src1=img,alpha=0.8,src2=gray_back,beta=0.4,gamma=10)\n",
    "        return blended\n",
    "\n",
    "    def random_transformation(img):\n",
    "        if np.random.rand()<0.5:\n",
    "            img=fuse_gray(img)\n",
    "        elif np.random.rand()<0.5:\n",
    "            img=random_brightness(img)\n",
    "        elif np.random.rand()<0.5:\n",
    "            img=dilation(img)\n",
    "        elif np.random.rand()<0.5:\n",
    "            img=erosion(img)\n",
    "\n",
    "        else:\n",
    "            img=np.array(img)\n",
    "        return Image.fromarray(img)\n",
    "\n",
    "    file=open('annotation.txt','a+')\n",
    "\n",
    "    file_counter=0\n",
    "    progress(0, desc=\"Starting...\")\n",
    "    for i in progress.tqdm(range(int(n_samples))):\n",
    "        back_c=random.choice(backgroud_list).copy()\n",
    "        start_cap=random.choice(capitalletters)\n",
    "        filename=''.join([random.choice(smallletters) for c in range(random.choice([5,6,7,8,9,10,11]))])\n",
    "        random_font=random.choice(fonts_list)\n",
    "        try :\n",
    "            font=ImageFont.truetype(random_font,size=random.choice(font_size))\n",
    "        except :\n",
    "            print(random_font)\n",
    "        if word_type=='all':\n",
    "            word=''.join([random.choice(all_char) for b in range(random.choice(word_lengths))])\n",
    "        elif word_type=='lowercase':\n",
    "            word=''.join([random.choice(smallletters) for b in range(random.choice(word_lengths))])\n",
    "        elif word_type=='uppercase':\n",
    "            word=''.join([random.choice(capitalletters) for b in range(random.choice(word_lengths))])\n",
    "        elif word_type=='firstcapital':\n",
    "            word=''.join([random.choice(smallletters) for b in range(random.choice(word_lengths)-1)])\n",
    "            word=start_cap+word\n",
    "        elif word_type=='digits':\n",
    "            word=''.join([random.choice(digits) for b in range(random.choice(word_lengths))])\n",
    "        elif word_type=='punctuation':\n",
    "            word=''.join([random.choice(smallletters) for b in range(random.choice(word_lengths))])\n",
    "            word=word+str(random.choice(punclist))\n",
    "        else:\n",
    "            raise Exception(\"Invalid word choice.\")\n",
    "            \n",
    "        left, top, right, bottom = font.getbbox(word) \n",
    "        w = right - left\n",
    "        h = bottom  \n",
    "        \n",
    "        back_c=back_c.resize((w+5,h+5))\n",
    "        draw=ImageDraw.Draw(back_c)\n",
    "        draw.text((0,0),text=word,font=font,fill='rgb(0,0,0)')\n",
    "        back_c=random_transformation(back_c)\n",
    "        back_c.save(f'./images/{file_counter}_{filename}.jpg')\n",
    "        #print(f'./images/{file_counter}_{filename}.jpg   ', random_font)\n",
    "        file.writelines(str(file_counter)+'_'+filename+'.jpg'+'\\t'+word+'\\n')\n",
    "        file_counter+=1\n",
    "    return \"Done !\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crée une liste des lettres et chiffres existants\n",
    "punclist=',.?!:;'\n",
    "char_list = string.ascii_letters+string.digits+punclist\n",
    "\n",
    "# Transforme les mots en liste de chiffres  \n",
    "def encode_to_labels(txt):\n",
    "    # encoding each output word into digits\n",
    "    # Crée une liste pour l'output\n",
    "    dig_lst = []\n",
    "    # Pour chaque caractère dans le mot :\n",
    "    for index, char in enumerate(txt):\n",
    "        try:\n",
    "            # Ajouter dans la liste \"dig_lst\" le numéro d'index du caractère\n",
    "            dig_lst.append(char_list.index(char))\n",
    "        except:\n",
    "            print(char)       \n",
    "    return dig_lst\n",
    "\n",
    "# Trouve la couleur dominante d'une image\n",
    "def find_dominant_color(image):\n",
    "        # Resizing parameters\n",
    "        width, height = 150,150\n",
    "        # Fait une interpolation pour changer la taille de l'image source\n",
    "        image = image.resize((width, height),resample = 0)\n",
    "        # Get colors from image object\n",
    "        pixels = image.getcolors(width * height)\n",
    "        # Sort them by count number(first element of tuple)\n",
    "        sorted_pixels = sorted(pixels, key=lambda t: t[0])\n",
    "        # Get the most frequent color\n",
    "        dominant_color = sorted_pixels[-1][1]\n",
    "        return dominant_color\n",
    "\n",
    "def preprocess_img(img, imgSize):\n",
    "    \"put img into target img of size imgSize, transpose for TF and normalize gray-values\"\n",
    "\n",
    "    # there are damaged files in IAM dataset - just use black image instead\n",
    "    if img is None:\n",
    "        img = np.zeros([imgSize[1], imgSize[0]]) \n",
    "        print(\"Image None!\")\n",
    "\n",
    "    # create target image and copy sample image into it\n",
    "    # Récupère la largeur et hauteur attendue \"wt\" pour Width Target et \"ht\" pour Height Target\n",
    "    (wt, ht) = imgSize\n",
    "    # Récupère la hauteur et largeur de l'image source\n",
    "    (h, w) = img.shape\n",
    "    # Fait le rapport de la largeur sur la largeur attendue\n",
    "    fx = w / wt\n",
    "    # Fait le rapport de la hauteur sur la hauteur attendue\n",
    "    fy = h / ht\n",
    "    # Prend le rapport le plus grand des deux\n",
    "    f = max(fx, fy)\n",
    "    # Définie la nouvelle taille de l'image en prennant le plus grand entre :\n",
    "    # - Le plus petit entre la largeur attendue et la largeur actuelle divisée par le rapport f\n",
    "    # - 1\n",
    "    newSize = (max(min(wt,\n",
    "                       int(w / f)), \n",
    "                   1),\n",
    "               max(min(ht,\n",
    "                       int(h / f)),\n",
    "                   1)\n",
    "              )  \n",
    "    # scale according to f (result at least 1 and at most wt or ht)\n",
    "    # Fait une interpolation pour changer la taille de l'image vers la nouvelle taille\n",
    "    img = cv2.resize(img, newSize, interpolation=cv2.INTER_CUBIC) # INTER_CUBIC interpolation best approximate the pixels image\n",
    "                                                                  # see this https://stackoverflow.com/a/57503843/7338066\n",
    "\n",
    "    # Trouve la couleur dominante\n",
    "    most_freq_pixel=find_dominant_color(Image.fromarray(img))\n",
    "    # Fait un applat de la couleur dominante\n",
    "    target = np.ones([ht, wt]) * most_freq_pixel\n",
    "    # Fusionner l'applat et l'image dont la taille a été changée pour faire de l'érodage\n",
    "    target[0:newSize[1], 0:newSize[0]] = img\n",
    "\n",
    "    img = target\n",
    "    return img\n",
    "\n",
    "def CRNN_train(batch_size = 256,\n",
    "               epochs = 15,\n",
    "               learning_rate = 0.0001,\n",
    "               split = 10,\n",
    "               decay = True,\n",
    "               nbr_smpl_train = 150000,\n",
    "               progress=gr.Progress()\n",
    "              ):\n",
    "    batch_size = int(batch_size)\n",
    "    epochs = int(epochs)\n",
    "    learning_rate = float(learning_rate)\n",
    "    split = int(split)\n",
    "    nbr_smpl = int(nbr_smpl_train)\n",
    "    #lists for training dataset\n",
    "    training_img = []\n",
    "    training_txt = []\n",
    "    train_input_length = []\n",
    "    train_label_length = []\n",
    "    orig_txt = []\n",
    "\n",
    "    #lists for validation dataset\n",
    "    valid_img = []\n",
    "    valid_txt = []\n",
    "    valid_input_length = []\n",
    "    valid_label_length = []\n",
    "    valid_orig_txt = []\n",
    "\n",
    "    max_label_len = 0\n",
    "\n",
    "    # Ouvre le fichier annotation.txt qui contient le nom des images , le contenu des images\n",
    "    annot=open('./annotation.txt','r').readlines()\n",
    "    imagenames=[]\n",
    "    txts=[]\n",
    "\n",
    "    # Récupère les données dans annotation.txt et répartis en deux listes\n",
    "    for cnt in annot:\n",
    "        filename,txt=cnt.split('\\t')[0],cnt.split('\\t')[1].split('\\n')[0]\n",
    "        imagenames.append(filename)\n",
    "        txts.append(txt)\n",
    "\n",
    "\n",
    "    # Mélange les données pour un seed différent à chaque fois\n",
    "    c = list(zip(imagenames, txts))\n",
    "    random.shuffle(c)\n",
    "    imagenames, txts = zip(*c)\n",
    "\n",
    "\n",
    "    # Pour chaque image :\n",
    "    #for i in progress.tqdm(range(len(imagenames))):\n",
    "    for i in progress.tqdm(range(nbr_smpl)):\n",
    "            # Lis l'image\n",
    "            img = cv2.imread('./images/'+imagenames[i],0)   \n",
    "            # Effectue le preprocess sur l'image\n",
    "            img=preprocess_img(img,(128,32))\n",
    "            # Augmente la profondeur de l'array d'un niveau. [a] -> [[a]]\n",
    "            img=np.expand_dims(img,axis=-1)\n",
    "            # Diviser les valeurs des pixels par 255 pour avoir une representation 0-1\n",
    "            img = img/255\n",
    "            # Récupere la légende correspondante\n",
    "            txt = txts[i]\n",
    "\n",
    "            # Compute maximum length of the text\n",
    "            if len(txt) > max_label_len:\n",
    "                max_label_len = len(txt)\n",
    "\n",
    "\n",
    "            # Split the 150000 data into validation and training dataset as 10% and 90% respectively\n",
    "            if i%split == 0:     \n",
    "                valid_orig_txt.append(txt)   \n",
    "                valid_label_length.append(len(txt))\n",
    "                valid_input_length.append(31)\n",
    "                valid_img.append(img)\n",
    "                valid_txt.append(encode_to_labels(txt))\n",
    "            else:\n",
    "                orig_txt.append(txt)   \n",
    "                train_label_length.append(len(txt))\n",
    "                train_input_length.append(31)\n",
    "                training_img.append(img)\n",
    "                training_txt.append(encode_to_labels(txt)) \n",
    "\n",
    "            # Break the loop if total data is nbr_smpl\n",
    "            if i == nbr_smpl:\n",
    "                flag = 1\n",
    "                break\n",
    "            i+=1\n",
    "\n",
    "    # Pad each output label to maximum text length\n",
    "    # Ajoute un chiffre non-utilisé dans la liste de caractères derrière les légendes pour qu'elles aient toutes la taille maximale\n",
    "    train_padded_txt = pad_sequences(training_txt, maxlen=max_label_len, padding='post', value = len(char_list))\n",
    "    valid_padded_txt = pad_sequences(valid_txt, maxlen=max_label_len, padding='post', value = len(char_list))\n",
    "\n",
    "    inputs = Input(shape=(32,128,1))\n",
    "\n",
    "    # convolution layer with kernel size (3,3)\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D\n",
    "    # \"relu\" = max(0,x)\n",
    "    conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs)\n",
    "    # poolig layer with kernel size (2,2)\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D\n",
    "    pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)\n",
    "\n",
    "    conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)\n",
    "    pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
    "\n",
    "    conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)\n",
    "\n",
    "    conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)\n",
    "    # poolig layer with kernel size (2,1)\n",
    "    pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)\n",
    "\n",
    "    conv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(pool_4)\n",
    "\n",
    "    # Batch normalization layer\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization\n",
    "    batch_norm_5 = BatchNormalization()(conv_5)\n",
    "\n",
    "    conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\n",
    "    batch_norm_6 = BatchNormalization()(conv_6)\n",
    "    pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)\n",
    "\n",
    "    conv_7 = Conv2D(512, (2,2), activation = 'relu')(pool_6)\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda \n",
    "    squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)\n",
    "\n",
    "    # bidirectional LSTM layers with units=128\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional\n",
    "    blstm_1 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(squeezed)\n",
    "    blstm_2 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(blstm_1)\n",
    "\n",
    "    outputs = Dense(len(char_list)+1, activation = 'softmax')(blstm_2)\n",
    "\n",
    "    # model to be used at test time\n",
    "    act_model = Model(inputs, outputs)\n",
    "\n",
    "    labels = Input(name='the_labels', shape=[max_label_len], dtype='float32')\n",
    "    input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "    label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "\n",
    "    def ctc_lambda_func(args):\n",
    "        y_pred, labels, input_length, label_length = args\n",
    "\n",
    "        return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([outputs, labels, input_length, label_length])\n",
    "\n",
    "    #model to be used at training time\n",
    "    model = Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out)\n",
    "    \n",
    "    if decay == True :\n",
    "        initial_learning_rate = learning_rate\n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate,\n",
    "            decay_steps=100000,\n",
    "            decay_rate=0.96,\n",
    "            staircase=True)\n",
    "    else :\n",
    "        lr_schedule = learning_rate\n",
    "        \n",
    "    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule), run_eagerly=True)\n",
    "    \n",
    "    filepath=\"./best_model.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    # Conversion en array\n",
    "    training_img = np.array(training_img)\n",
    "    train_input_length = np.array(train_input_length)\n",
    "    train_label_length = np.array(train_label_length)\n",
    "\n",
    "    valid_img = np.array(valid_img)\n",
    "    valid_input_length = np.array(valid_input_length)\n",
    "    valid_label_length = np.array(valid_label_length)\n",
    "                                 \n",
    "    yield \"Génération de modèle en cours...\"\n",
    "    model.fit(x=[training_img, train_padded_txt, train_input_length, train_label_length], y=np.zeros(len(training_img)), batch_size=batch_size, epochs = epochs, validation_data = ([valid_img, valid_padded_txt, valid_input_length, valid_label_length], [np.zeros(len(valid_img))]), verbose = 1, callbacks = callbacks_list)\n",
    "    return \"Done !\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_input, \n",
    "             nbr_smpl_eval, \n",
    "             progress=gr.Progress()\n",
    "            ):\n",
    "    \n",
    "    nbr_smpl = int(nbr_smpl_eval)\n",
    "    inputs = Input(shape=(32,128,1))\n",
    "    conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs)\n",
    "    pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)\n",
    "    conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)\n",
    "    pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
    "    conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)\n",
    "    conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)\n",
    "    pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)\n",
    "    conv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(pool_4)\n",
    "    batch_norm_5 = BatchNormalization()(conv_5)\n",
    "    conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\n",
    "    batch_norm_6 = BatchNormalization()(conv_6)\n",
    "    pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)\n",
    "    conv_7 = Conv2D(512, (2,2), activation = 'relu')(pool_6)\n",
    "    squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)\n",
    "\n",
    "    blstm_1 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(squeezed)\n",
    "    blstm_2 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(blstm_1)\n",
    "\n",
    "    outputs = Dense(len(char_list)+1, activation = 'softmax')(blstm_2)\n",
    "    \n",
    "    #lists for training dataset\n",
    "    training_img = []\n",
    "    training_txt = []\n",
    "    train_input_length = []\n",
    "    train_label_length = []\n",
    "    orig_txt = []\n",
    "\n",
    "    max_label_len = 0\n",
    "\n",
    "    # Ouvre le fichier annotation.txt qui contient le nom des images , le contenu des images\n",
    "    annot=open('./annotation.txt','r').readlines()\n",
    "    imagenames=[]\n",
    "    txts=[]\n",
    "\n",
    "    # Récupère les données dans annotation.txt et répartis en deux listes\n",
    "    for cnt in annot:\n",
    "        filename,txt=cnt.split(',')[0],cnt.split(',')[1].split('\\n')[0]\n",
    "        imagenames.append(filename)\n",
    "        txts.append(txt)\n",
    "\n",
    "    # Mélange les données pour un seed différent à chaque fois\n",
    "    c = list(zip(imagenames, txts))\n",
    "    random.shuffle(c)\n",
    "    imagenames, txts = zip(*c)\n",
    "    # Pour chaque image :\n",
    "    for i in progress.tqdm(range(nbr_smpl)):\n",
    "        # Lis l'image\n",
    "        img = cv2.imread('./images/'+imagenames[i],0)   \n",
    "        # Effectue le preprocess sur l'image\n",
    "        img=preprocess_img(img,(128,32))\n",
    "        # Augmente la profondeur de l'array d'un niveau. [a] -> [[a]]\n",
    "        img=np.expand_dims(img,axis=-1)\n",
    "        # Diviser les valeurs des pixels par 255 pour avoir une representation 0-1\n",
    "        img = img/255\n",
    "        # Récupere la légende correspondante\n",
    "        txt = txts[i]\n",
    "\n",
    "        # Compute maximum length of the text\n",
    "        if len(txt) > max_label_len:\n",
    "            max_label_len = len(txt)\n",
    "\n",
    "        orig_txt.append(txt)   \n",
    "        train_label_length.append(len(txt))\n",
    "        train_input_length.append(31)\n",
    "        training_img.append(img)\n",
    "        training_txt.append(encode_to_labels(txt)) \n",
    "    \n",
    "    labels = Input(name='the_labels', shape=[max_label_len], dtype='float32')\n",
    "    input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "    label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "    \n",
    "    def ctc_lambda_func(args):\n",
    "        y_pred, labels, input_length, label_length = args\n",
    "        return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([outputs, labels, input_length, label_length])\n",
    "    act_model = Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out)\n",
    "    act_model.load_weights(model_input.orig_name)\n",
    "    \n",
    "    training_img = np.array(training_img)\n",
    "    train_input_length = np.array(train_input_length)\n",
    "    train_label_length = np.array(train_label_length)\n",
    "\n",
    "    train_padded_txt = pad_sequences(training_txt, maxlen=max_label_len, padding='post', value = len(char_list))\n",
    "    # loss=\"kullback_leibler_divergence\"\n",
    "    yield \"Calcul en cours...\"\n",
    "    act_model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = 'adam')\n",
    "    x=[training_img, train_padded_txt, train_input_length, train_label_length]\n",
    "    y=np.zeros(len(training_img))\n",
    "    scores = act_model.evaluate(x, y, verbose=1)\n",
    "\n",
    "    return \"%s : %.2f\" % (act_model.metrics_names[0], scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize(img,language):\n",
    "    #Open image and segment into lines\n",
    "    try : \n",
    "        img = Image.open(img)\n",
    "    except:\n",
    "        img = img\n",
    "    img = img.save(\"temp.jpg\")\n",
    "    \n",
    "    line_img_array=segment_into_lines(\"temp.jpg\")\n",
    "    \n",
    "    #Creating lists to store the line indexes,words list.\n",
    "    full_index_indicator=[]\n",
    "    all_words_list=[]\n",
    "    #Variable to count the total no of lines in page.\n",
    "    len_line_arr=0\n",
    "    \n",
    "    yield \"Détection en cours...\"\n",
    "    #Segment the lines into words and store as arrays.\n",
    "    for idx,im in enumerate(line_img_array):\n",
    "        line_indicator,word_array=segment_into_words(im,idx)\n",
    "        for k in range(len(word_array)):\n",
    "            full_index_indicator.append(line_indicator[k])\n",
    "            all_words_list.append(word_array[k])\n",
    "        len_line_arr+=1\n",
    "\n",
    "    all_words_list=np.array(all_words_list)\n",
    "\n",
    "    yield \"Reconnaissance des mots en cours...\"\n",
    "    #Perform the recognition on list of list of words.\n",
    "    recognize_words(full_index_indicator,all_words_list,len_line_arr,language)\n",
    "    \n",
    "    file=open('recognized_texts.txt','r')\n",
    "    pred=[]\n",
    "    for i in file:\n",
    "        pred.append(i)\n",
    "    yield pred\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "kw6mKnt4MsqR",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7865\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\h.decure\\Anaconda3\\lib\\site-packages\\gradio\\routes.py\", line 384, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"C:\\Users\\h.decure\\Anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1024, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"C:\\Users\\h.decure\\Anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 850, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"C:\\Users\\h.decure\\Anaconda3\\lib\\site-packages\\anyio\\to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "  File \"C:\\Users\\h.decure\\Anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"C:\\Users\\h.decure\\Anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\h.decure\\Anaconda3\\lib\\site-packages\\gradio\\utils.py\", line 445, in async_iteration\n",
      "    return next(iterator)\n",
      "  File \"C:\\Users\\h.decure\\AppData\\Local\\Temp\\ipykernel_11204\\302796456.py\", line 252, in CRNN_train\n",
      "    model.fit(x=[training_img, train_padded_txt, train_input_length, train_label_length], y=np.zeros(len(training_img)), batch_size=batch_size, epochs = epochs, validation_data = ([valid_img, valid_padded_txt, valid_input_length, valid_label_length], [np.zeros(len(valid_img))]), verbose = 1, callbacks = callbacks_list)\n",
      "  File \"C:\\Users\\h.decure\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"C:\\Users\\h.decure\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1420, in fit\n",
      "    raise ValueError('Unexpected result of `train_function` '\n",
      "ValueError: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"OCR From Scratch\")\n",
    "    \n",
    "    with gr.Tab(\"Generate Data\"):\n",
    "        n_samples = gr.Number(value=150000)\n",
    "        word_type = gr.Radio(\n",
    "        ['all','lowercase', 'uppercase', 'firstcapital', 'digits', 'punctuation'], label=\"Type\" , value = 'all'\n",
    "        )\n",
    "        generate_button = gr.Button(\"GENERATE\")\n",
    "        generate_output = gr.Textbox(show_label=False,interactive=False)\n",
    "        \n",
    "    with gr.Tab(\"Train CRNN\"):\n",
    "        batch_size = gr.Number(value=256, label=\"batch_size\")\n",
    "        epochs = gr.Number(value=15, label=\"epochs\")\n",
    "        learning_rate = gr.Slider(0.000000000001, 0.0001, value=0.0001, step=0.000000000001, label = \"Learning Rate\")\n",
    "        decay = gr.Checkbox(value=True, label=\"Activer le Decay du Learning Rate\")\n",
    "        split = gr.Slider(0, 100, value=10, interactive=True, label = \"Pourcentage du Dataset à utiliser pour Test\")\n",
    "        maximum = 0\n",
    "        for path in os.listdir(\"./images/\"):\n",
    "                maximum += 1\n",
    "        nbr_smpl_train = gr.Slider(0, maximum, value=maximum, interactive=True, step=100, label = \"Nombres d'images du Dataset à utiliser\")\n",
    "        CRNN_button = gr.Button(\"TRAIN\")\n",
    "        train_text_output = gr.Textbox(show_label=False,interactive=False)\n",
    "        \n",
    "    with gr.Tab(\"Evaluate\"):\n",
    "        model_input = gr.File(label=\"Modèle à évaluer\")\n",
    "        maximum = 0\n",
    "        for path in os.listdir(\"./images/\"):\n",
    "                maximum += 1\n",
    "        nbr_smpl_eval = gr.Slider(0, maximum, value=maximum, interactive=True, step=100, label = \"Nombres d'images du Dataset à utiliser\")\n",
    "        evaluate_button = gr.Button(\"Evaluate\")\n",
    "        eval_text_output = gr.Textbox(show_label=False,interactive=False)\n",
    "    \n",
    "    with gr.Tab(\"Recognize\"):\n",
    "        image_input = gr.Image(type=\"filepath\")\n",
    "        language = gr.Radio(['en','fr','es','de','pt'], label = 'Language', value = 'en', interactive=True)\n",
    "        recognize_button = gr.Button(\"Recognize\")\n",
    "        recon_text_output = gr.Textbox(show_label=False,interactive=False)\n",
    "    \n",
    "    with gr.Tab(\"Live\"):\n",
    "        with gr.Row():\n",
    "            cam_input = gr.Image(source=\"webcam\", mirror_webcam=False, type = \"pil\")\n",
    "            cam_text_output = gr.Textbox(show_label=False,interactive=False)\n",
    "        language = gr.Radio(['en','fr','es','de','pt'], label = 'Language', value = 'en', interactive=True)\n",
    "        live_button = gr.Button(\"Recognize\")\n",
    "      \n",
    "    generate_button.click(generate_data, inputs=[n_samples, word_type], outputs=generate_output)\n",
    "    CRNN_button.click(CRNN_train, inputs=[batch_size, epochs, learning_rate, decay, split, nbr_smpl_train], outputs=train_text_output)\n",
    "    evaluate_button.click(evaluate, inputs=[model_input, nbr_smpl_eval], outputs=eval_text_output)\n",
    "    recognize_button.click(recognize, inputs=[image_input,language], outputs=recon_text_output)\n",
    "    live_button.click(recognize, inputs=[cam_input,language], outputs=cam_text_output)\n",
    "\n",
    "demo.queue().launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CRNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
