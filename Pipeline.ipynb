{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ab3be0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import string\n",
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from spellchecker import SpellChecker\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "spell = SpellChecker()\n",
    "#spell = SpellChecker(language='fr')\n",
    "\n",
    "\n",
    "char_list = string.ascii_letters+string.digits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inputs = Input(shape=(32,128,1))\n",
    " \n",
    "\n",
    "conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs)\n",
    "\n",
    "pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)\n",
    " \n",
    "conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)\n",
    "pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
    " \n",
    "conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)\n",
    " \n",
    "conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)\n",
    "\n",
    "pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)\n",
    " \n",
    "conv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(pool_4)\n",
    "\n",
    "batch_norm_5 = BatchNormalization()(conv_5)\n",
    " \n",
    "conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\n",
    "batch_norm_6 = BatchNormalization()(conv_6)\n",
    "pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)\n",
    " \n",
    "conv_7 = Conv2D(512, (2,2), activation = 'relu')(pool_6)\n",
    " \n",
    "squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)\n",
    " \n",
    "\n",
    "blstm_1 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(squeezed)\n",
    "blstm_2 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(blstm_1)\n",
    " \n",
    "outputs = Dense(len(char_list)+1, activation = 'softmax')(blstm_2)\n",
    "\n",
    "\n",
    "act_model = Model(inputs, outputs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "act_model.load_weights('CRNN_model.hdf5')\n",
    "\n",
    "def find_dominant_color(image):\n",
    "        #Resizing parameters\n",
    "        width, height = 150,150\n",
    "        image = image.resize((width, height),resample = 0)\n",
    "        #Get colors from image object\n",
    "        pixels = image.getcolors(width * height)\n",
    "        #Sort them by count number(first element of tuple)\n",
    "        sorted_pixels = sorted(pixels, key=lambda t: t[0])\n",
    "        #Get the most frequent color\n",
    "        dominant_color = sorted_pixels[-1][1]\n",
    "        return dominant_color\n",
    "\n",
    "def preprocess_img(img, imgSize):\n",
    "    \"put img into target img of size imgSize, transpose for TF and normalize gray-values\"\n",
    "\n",
    "    # there are damaged files in IAM dataset - just use black image instead\n",
    "    if img is None:\n",
    "        img = np.zeros([imgSize[1], imgSize[0]]) \n",
    "        print(\"Image None!\")\n",
    "\n",
    "    # create target image and copy sample image into it\n",
    "    (wt, ht) = imgSize\n",
    "    (h, w) = img.shape\n",
    "    fx = w / wt\n",
    "    fy = h / ht\n",
    "    f = max(fx, fy)\n",
    "    newSize = (max(min(wt, int(w / f)), 1),\n",
    "               max(min(ht, int(h / f)), 1))  # scale according to f (result at least 1 and at most wt or ht)\n",
    "    img = cv2.resize(img, newSize, interpolation=cv2.INTER_CUBIC) # INTER_CUBIC interpolation best approximate the pixels image\n",
    "                                                               # see this https://stackoverflow.com/a/57503843/7338066\n",
    "    most_freq_pixel=find_dominant_color(Image.fromarray(img))\n",
    "    target = np.ones([ht, wt]) * most_freq_pixel  \n",
    "    target[0:newSize[1], 0:newSize[0]] = img\n",
    "\n",
    "    img = target\n",
    "\n",
    "    return img\n",
    "\n",
    "def pad_img(img):\n",
    "    old_h,old_w=img.shape[0],img.shape[1]\n",
    "\n",
    "    #Pad the height.\n",
    "\n",
    "    #If height is less than 512 then pad to 512\n",
    "    if old_h<512:\n",
    "        to_pad=np.ones((512-old_h,old_w))*255\n",
    "        img=np.concatenate((img,to_pad))\n",
    "        new_height=512\n",
    "    else:\n",
    "    #If height >512 then pad to nearest 10.\n",
    "        to_pad=np.ones((roundup(old_h)-old_h,old_w))*255\n",
    "        img=np.concatenate((img,to_pad))\n",
    "        new_height=roundup(old_h)\n",
    "\n",
    "    #Pad the width.\n",
    "    if old_w<512:\n",
    "        to_pad=np.ones((new_height,512-old_w))*255\n",
    "        img=np.concatenate((img,to_pad),axis=1)\n",
    "        new_width=512\n",
    "    else:\n",
    "        to_pad=np.ones((new_height,roundup(old_w)-old_w))*255\n",
    "        img=np.concatenate((img,to_pad),axis=1)\n",
    "        new_width=roundup(old_w)-old_w\n",
    "    return img\n",
    "\n",
    "def roundup(x):\n",
    "    return int(math.ceil(x / 10.0)) * 10\n",
    "\n",
    "def unet_page(pretrained_weights = None,input_size = (512,512,1)):\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs,conv10)\n",
    "\n",
    "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "\n",
    "    if(pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "\n",
    "    return model\n",
    "\n",
    "def recognize_words(line_indicator,word_array,n_lines):\n",
    "\n",
    "    file=open('recognized_texts.txt','w+')\n",
    "\n",
    "    line_rec=[]\n",
    "    for listidx in range(n_lines):\n",
    "        line_rec.append([])\n",
    "\n",
    "\n",
    "    predictions=act_model.predict(word_array)\n",
    "   \n",
    "\n",
    "    out = K.get_value(K.ctc_decode(predictions, input_length=np.ones(predictions.shape[0])*predictions.shape[1],\n",
    "                         greedy=True)[0][0])\n",
    "\n",
    "    lw_idx=0\n",
    "   \n",
    "    for wordidxs in out:\n",
    "        word=[]\n",
    "        for char in wordidxs:\n",
    "            if int(char)!=-1:\n",
    "                word.append(char_list[int(char)])\n",
    "        try :\n",
    "            word=spell.correction(''.join(word))\n",
    "        except :\n",
    "            word=''.join(word)\n",
    "        if word != None :\n",
    "            line_rec[line_indicator[lw_idx]].append(word)\n",
    "        lw_idx+=1\n",
    "\n",
    "    for listidx in range(n_lines):\n",
    "        line=' '.join(line_rec[listidx])\n",
    "        print(line)\n",
    "        file.writelines(line+'\\n')\n",
    "    file.close()\n",
    "\n",
    "def segment_into_lines(filename):\n",
    "    model=unet_page()\n",
    "    model.load_weights('text_seg_model.h5')\n",
    "    \n",
    "    line_img_array=[]\n",
    "\n",
    "    img=cv2.imread(f'{filename}',0)\n",
    "    ret,img=cv2.threshold(img,150,255,cv2.THRESH_BINARY_INV)\n",
    "    img=cv2.resize(img,(512,512))\n",
    "   \n",
    "    img= np.expand_dims(img,axis=-1)\n",
    "\n",
    "    img=np.expand_dims(img,axis=0)\n",
    "    pred=model.predict(img)\n",
    "    pred=np.squeeze(np.squeeze(pred,axis=0),axis=-1)\n",
    "\n",
    "    \n",
    "\n",
    "    coordinates=[]\n",
    "    img = cv2.normalize(src=pred, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)\n",
    "    cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU,img)\n",
    "    ori_img=cv2.imread(f'{filename}',0)\n",
    " \n",
    "\n",
    "    (H, W) = ori_img.shape[:2]\n",
    "    (newW, newH) = (512, 512)\n",
    "    rW = W / float(newW)\n",
    "    rH = H / float(newH)\n",
    "    \n",
    "    contours, hier = cv2.findContours(img, cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n",
    "    for c in contours:\n",
    "        # get the bounding rect\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        #cv2.rectangle(ori_img, (int(x*rW), int(y*rH)), (int((x+w)*rW),int((y+h)*rH)), (255,0,0), 1)\n",
    "        coordinates.append((int(x*rW),int(y*rH),int((x+w)*rW),int((y+h)*rH)))\n",
    "    #cv2.imwrite(\"output.jpg\",ori_img)\n",
    "\n",
    "    for i in range(len(coordinates)-1,-1,-1):\n",
    "        coors=coordinates[i]\n",
    "\n",
    "        p_img=ori_img[coors[1]:coors[3],coors[0]:coors[2]].copy()\n",
    "\n",
    "        line_img_array.append(p_img)\n",
    "\n",
    "    return line_img_array\n",
    "\n",
    "def sort_word(wordlist):\n",
    "    wordlist.sort(key=lambda x:x[0])\n",
    "    return wordlist\n",
    "\n",
    "def unet(pretrained_weights = None,input_size = (512,512,1)):\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs,conv10)\n",
    "\n",
    "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "\n",
    "    if(pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "\n",
    "    return model\n",
    "\n",
    "def segment_into_words(line_img,idx):\n",
    "    \"\"\"This function takes in the line image and line index returns word images and the reference\n",
    "    of line they belong to.\"\"\"\n",
    "    model=unet()\n",
    "    model.load_weights('word_seg_model.h5')\n",
    "\n",
    "    img=pad_img(line_img)\n",
    "    ori_img=img.copy()\n",
    "    #ori_img=np.stack((ori_img,)*3, axis=-1)\n",
    "    ret,img=cv2.threshold(img,150,255,cv2.THRESH_BINARY_INV)\n",
    "    \n",
    "    img=cv2.resize(img,(512,512))\n",
    "    img=np.expand_dims(img,axis=-1)\n",
    "    img=img/255\n",
    "    img=np.expand_dims(img,axis=0)\n",
    "    seg_pred=model.predict(img)\n",
    "    seg_pred=np.squeeze(np.squeeze(seg_pred,axis=0),axis=-1)\n",
    "    seg_pred=cv2.normalize(src=seg_pred, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)\n",
    "    cv2.threshold(seg_pred,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU,seg_pred)\n",
    "    contours, hier = cv2.findContours(seg_pred, cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n",
    "    \n",
    "    (H, W) = ori_img.shape[:2]\n",
    "    (newW, newH) = (512, 512)\n",
    "    rW = W / float(newW)\n",
    "    rH = H / float(newH)\n",
    "\n",
    "    coordinates=[]\n",
    "\n",
    "    for c in contours:\n",
    "        # get the bounding rect\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        # draw a white rectangle to visualize the bounding rect\n",
    "        # cv2.rectangle(ori_img, (int(x*rW), int(y*rH)), (int((x+w)*rW),int((y+h)*rH)), (255,0,0), 1)\n",
    "        coordinates.append((int(x*rW),int(y*rH),int((x+w)*rW),int((y+h)*rH)))\n",
    "\n",
    "    coordinates=sort_word(coordinates)  #Sorting according to x-coordinates.\n",
    "    word_counter=0\n",
    "\n",
    "    word_array=[]\n",
    "    line_indicator=[]\n",
    "\n",
    "    for (x1,y1,x2,y2) in coordinates:\n",
    "        word_img=ori_img[y1:y2,x1:x2]\n",
    "        word_img=preprocess_img(word_img,(128,32))\n",
    "        word_img=np.expand_dims(word_img,axis=-1)\n",
    "        word_array.append(word_img)\n",
    "        line_indicator.append(idx)\n",
    "\n",
    "    return line_indicator,word_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d4087ae1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "4/4 [==============================] - 1s 17ms/step\n",
      "over the last three vears of ou relationship tax has\n",
      "grown immense as a person die has worked vith me\n",
      "gal i\n",
      "individual and during couples counseling to become a\n",
      "better communicate be more patient and less\n",
      "i i\n",
      "when e are in a disagreement prior to\n",
      "counseling hed resort to elfin cussing and calling me\n",
      "names her e argued chere vere also a handful of\n",
      "incidents when hed been drinking that he threw and\n",
      "broke things all over the house my home that i on\n",
      "sole but ve live together these explosive incidents\n",
      "have not happened since he vowed to stop drinking a\n",
      "hear also\n",
      "i i\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "#Open image and segment into lines\n",
    "line_img_array=segment_into_lines('test_image3.jpg')\n",
    "\n",
    "\n",
    "#Creating lists to store the line indexes,words list.\n",
    "full_index_indicator=[]\n",
    "all_words_list=[]\n",
    "#Variable to count the total no of lines in page.\n",
    "len_line_arr=0\n",
    "\n",
    "#Segment the lines into words and store as arrays.\n",
    "for idx,im in enumerate(line_img_array):\n",
    "    line_indicator,word_array=segment_into_words(im,idx)\n",
    "    for k in range(len(word_array)):\n",
    "        full_index_indicator.append(line_indicator[k])\n",
    "        all_words_list.append(word_array[k])\n",
    "    len_line_arr+=1\n",
    "    \n",
    "\n",
    "all_words_list=np.array(all_words_list)\n",
    "\n",
    "\n",
    "#Perform the recognition on list of list of words.\n",
    "recognize_words(full_index_indicator,all_words_list,len_line_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b4f4c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
